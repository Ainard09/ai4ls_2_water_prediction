{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNJGQl4nMScx",
    "outputId": "f27acd2c-589b-4aea-8d9f-ba115b90051a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skforecast\n",
      "  Downloading skforecast-0.12.1-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
      "Requirement already satisfied: numpy<1.27,>=1.20 in /usr/local/lib/python3.10/dist-packages (from skforecast) (1.26.4)\n",
      "Requirement already satisfied: pandas<2.3,>=1.2 in /usr/local/lib/python3.10/dist-packages (from skforecast) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn<1.5,>=1.2 in /usr/local/lib/python3.10/dist-packages (from skforecast) (1.3.2)\n",
      "Collecting optuna<3.7,>=2.10 (from skforecast)\n",
      "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: joblib<1.5,>=1.1 in /usr/local/lib/python3.10/dist-packages (from skforecast) (1.4.2)\n",
      "Collecting alembic>=1.5.0 (from optuna<3.7,>=2.10->skforecast)\n",
      "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna<3.7,>=2.10->skforecast)\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna<3.7,>=2.10->skforecast) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna<3.7,>=2.10->skforecast) (2.0.31)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna<3.7,>=2.10->skforecast) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=1.2->skforecast) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=1.2->skforecast) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=1.2->skforecast) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5,>=1.2->skforecast) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5,>=1.2->skforecast) (3.5.0)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna<3.7,>=2.10->skforecast)\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna<3.7,>=2.10->skforecast) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=1.2->skforecast) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna<3.7,>=2.10->skforecast) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna<3.7,>=2.10->skforecast) (2.1.5)\n",
      "Downloading skforecast-0.12.1-py3-none-any.whl (560 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m560.6/560.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna, skforecast\n",
      "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1 skforecast-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install skforecast tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_6SzvFyLFmFs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import skforecast\n",
    "from sklearn.feature_selection import RFECV\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.model_selection import bayesian_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "from skforecast.model_selection import select_features\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import shutil\n",
    "import re\n",
    "import traceback\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OSfT6fhxFmOQ"
   },
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "categorical_features = [\"weather\", \"season\"]\n",
    "transformer_exog = make_column_transformer(\n",
    "    (\n",
    "        OrdinalEncoder(\n",
    "            dtype=int,\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1,\n",
    "            encoded_missing_value=-1\n",
    "        ),\n",
    "        categorical_features\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "\n",
    "# Regressor hyperparameters search space\n",
    "def search_space(trial):\n",
    "\n",
    "  # Lags grid\n",
    "  lags_grid = tuple([12, 24, [1, 2, 3, 4, 7, 9, 24]])\n",
    "\n",
    "  search_space  = {\n",
    "      'max_iter'          : trial.suggest_int('max_iter', 400, 1200, step=100),\n",
    "      'max_depth'         : trial.suggest_int('max_depth', 3, 10, step=1),\n",
    "      'learning_rate'     : trial.suggest_float('learning_rate', 0.01, 1),\n",
    "      'min_samples_leaf'  : trial.suggest_int('min_samples_leaf', 1, 20, step=1),\n",
    "      'l2_regularization' : trial.suggest_float('l2_regularization', 0, 1),\n",
    "      'lags'              : trial.suggest_categorical('lags', lags_grid)\n",
    "  }\n",
    "  return search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "paOiln2fJPTx"
   },
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def smape(A, F):\n",
    "    tmp = 2 * np.abs(F - A) / (np.abs(A) + np.abs(F))\n",
    "    len_ = np.count_nonzero(~np.isnan(tmp))\n",
    "    if len_ == 0 and np.nansum(tmp) == 0: # Deals with a special case\n",
    "        return 100\n",
    "    return round(100 / len_ * np.nansum(tmp), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qJFpJzgfFmR5"
   },
   "outputs": [],
   "source": [
    "def search_hyperparameters(data, end_train, end_valid, exog_features,transformer_exog):\n",
    "\n",
    "  # instantiate a forcaster transformer with categorical features\n",
    "  forecaster = ForecasterAutoreg(\n",
    "  regressor = HistGradientBoostingRegressor(\n",
    "                  categorical_features=categorical_features,\n",
    "                  random_state=123\n",
    "              ),\n",
    "  lags = 24,\n",
    "  transformer_exog = transformer_exog\n",
    "  )\n",
    "\n",
    "  # search for best parameters\n",
    "  results_search, frozen_trial = bayesian_search_forecaster(\n",
    "  forecaster         = forecaster,\n",
    "  y                  = data.loc[:end_valid, 'gw-level'],\n",
    "  exog               = data.loc[:end_valid, exog_features],\n",
    "  search_space       = search_space,\n",
    "  steps              = 30,\n",
    "  refit              = False,\n",
    "  metric             = 'mean_absolute_percentage_error',\n",
    "  initial_train_size = len(data.loc[:end_train]),\n",
    "  fixed_train_size   = False,\n",
    "  n_trials           = 20,\n",
    "  random_state       = 123,\n",
    "  return_best        = True,\n",
    "  n_jobs             = 'auto',\n",
    "  verbose            = False,\n",
    "  show_progress      = True\n",
    "  )\n",
    "\n",
    "  best_params = results_search['params'].iat[0]\n",
    "\n",
    "  return best_params\n",
    "\n",
    "def train_and_predict(data, best_params, actual_data, end_valid, end_train, valid_num, train_num, df_idx, exog_features, transformer_exog):\n",
    "\n",
    "  # train for evaluation of the model\n",
    "  forecaster = ForecasterAutoreg(\n",
    "  regressor = HistGradientBoostingRegressor(**best_params,\n",
    "                  categorical_features=categorical_features,\n",
    "                  random_state=123\n",
    "              ),\n",
    "  lags = 24,\n",
    "  transformer_exog = transformer_exog\n",
    "  )\n",
    "\n",
    "  # train the model the time series train and validation dataset\n",
    "  forecaster.fit(\n",
    "    y    = data.loc[:end_train, 'gw-level'],\n",
    "    exog = data.loc[:end_train, exog_features]\n",
    "  )\n",
    "\n",
    "  # make predictions and evalute the model\n",
    "  predictions = forecaster.predict(\n",
    "      exog     = data.loc[df_idx[train_num+1]:, exog_features],\n",
    "      steps    = 26\n",
    "  )\n",
    "  df_preds = pd.DataFrame(predictions)\n",
    "  preds = df_preds[\"pred\"].values\n",
    "  # evaluate on symmetric mean absolute percentage error\n",
    "  smape_value = smape(actual_data, preds)\n",
    "\n",
    "  # train for future predictions\n",
    "  forecaster = ForecasterAutoreg(\n",
    "  regressor = HistGradientBoostingRegressor(**best_params,\n",
    "                  categorical_features=categorical_features,\n",
    "                  random_state=123\n",
    "              ),\n",
    "  lags = 24,\n",
    "  transformer_exog = transformer_exog\n",
    "  )\n",
    "  # train the model the time series train and validation dataset\n",
    "  forecaster.fit(\n",
    "    y    = data.loc[:end_valid, 'gw-level'],\n",
    "    exog = data.loc[:end_valid, exog_features]\n",
    "  )\n",
    "\n",
    "  # make predictions into the future\n",
    "  predictions = forecaster.predict(\n",
    "    exog     = data.loc[df_idx[valid_num+1]:, exog_features],\n",
    "    steps    = 26\n",
    "  )\n",
    "  df_preds = pd.DataFrame(predictions)\n",
    "\n",
    "  # free resources since it's going to run on iterations\n",
    "  del forecaster\n",
    "\n",
    "  return df_preds, smape_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GBQNJyBuFmY7"
   },
   "outputs": [],
   "source": [
    "def populate_test_data(data_dir):\n",
    "  preds_dict = {}\n",
    "  smape_dict = {}\n",
    "\n",
    "\n",
    "  # collect all files in the directory\n",
    "  filenames = os.listdir(data_dir)\n",
    "\n",
    "  try:\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "\n",
    "        hrbnz01 = filename.split(\".\")[0].split(\"-\")[-1]\n",
    "        filepath = path.join(data_dir, filename)\n",
    "        df_exog = pd.read_csv(filepath)\n",
    "        df_exog[\"season\"] = df_exog[\"season\"].astype(\"category\")\n",
    "        df_exog[\"weather\"] = df_exog[\"weather\"].astype(\"category\")\n",
    "        df_exog[\"date\"] = pd.to_datetime(df_exog[\"date\"])\n",
    "        df_exog.set_index(\"date\", inplace=True)\n",
    "        df_exog.index = pd.date_range(start=df_exog.index.min(), end=df_exog.index.max(), freq='MS')\n",
    "\n",
    "        # get the estimate end train and end validation dates\n",
    "        data = df_exog.copy()\n",
    "        exog_data = data.drop(\"gw-level\", axis=1)\n",
    "        exog_features = exog_data.columns\n",
    "        df_idx = data.index\n",
    "        train_num = int(len(data) * 0.8)\n",
    "        valid_num = len(data.loc[:\"2021-11-01\"])\n",
    "        end_train = df_idx[train_num]\n",
    "        end_valid = df_idx[valid_num]\n",
    "        end_evaluation = df_idx[train_num+26]\n",
    "        evaluate_data = data.loc[df_idx[train_num+1]: end_evaluation, \"gw-level\"].values\n",
    "\n",
    "\n",
    "        # tune for best hyperparamters and evaluate on MAPE metric\n",
    "        best_params = search_hyperparameters(data, end_train, end_valid, exog_features, transformer_exog)\n",
    "\n",
    "        # train and make predict into 26 months in the future of the test template\n",
    "        df_predictions, smape = train_and_predict(data,best_params, evaluate_data, end_valid, end_train, valid_num, train_num, df_idx, exog_features, transformer_exog)\n",
    "        df_predictions[\"pred\"] =  df_predictions[\"pred\"].round(2)\n",
    "        preds_dict[hrbnz01] = df_predictions['pred'].values\n",
    "        smape_dict[hrbnz01] = smape\n",
    "\n",
    "\n",
    "  except Exception as ex:\n",
    "    print(\"[Error]\")\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "  df_smape = pd.DataFrame(smape_dict, index=[0])\n",
    "  df_final_preds = pd.DataFrame(preds_dict, index=range(26))\n",
    "  df_final_preds.index = pd.date_range(start=pd.Timestamp(\"2021-12-01\") + pd.DateOffset(months=1), periods=26, freq='MS')\n",
    "  print(\"> Done\")\n",
    "\n",
    "  return df_final_preds, df_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dpcGTX_rf7a"
   },
   "outputs": [],
   "source": [
    "processed_data_dir = \"/content/drive/MyDrive/clean_processed_data_part1\"\n",
    "\n",
    "df_submission, df_smape = populate_test_data(processed_data_dir)\n",
    "\n",
    "df_submission.to_csv(\"df_submission.csv\")\n",
    "df_smape.to_csv(\"smape_score.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
