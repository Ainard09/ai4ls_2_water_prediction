{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install skforecast tqdm catboost"
      ],
      "metadata": {
        "id": "FNJGQl4nMScx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import sklearn\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import make_column_selector\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import skforecast\n",
        "from sklearn.feature_selection import RFECV\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "from skforecast.model_selection import bayesian_search_forecaster\n",
        "from skforecast.model_selection import backtesting_forecaster\n",
        "from skforecast.model_selection import select_features\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from os import path\n",
        "import shutil\n",
        "import re\n",
        "import traceback\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "_6SzvFyLFmFs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding\n",
        "transformer_exog = make_column_transformer(\n",
        "    (\n",
        "        OneHotEncoder(sparse_output=False, drop='if_binary'),\n",
        "        make_column_selector(dtype_exclude=np.number),\n",
        "    ),\n",
        "    remainder=\"passthrough\",\n",
        "    verbose_feature_names_out=False,\n",
        ").set_output(transform=\"pandas\")\n",
        "\n",
        "\n",
        "# Regressor hyperparameters search space\n",
        "def search_space(trial):\n",
        "  lags_grid = tuple([12, 24, [1, 2, 3, 4, 7, 9, 24]])\n",
        "  search_space  = {\n",
        "      'n_estimators'  : trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
        "      'max_depth'     : trial.suggest_int('max_depth', 3, 10, step=1),\n",
        "      'learning_rate' : trial.suggest_float('learning_rate', 0.01, 1),\n",
        "      'lags'          : trial.suggest_categorical('lags', lags_grid)\n",
        "  }\n",
        "  return search_space"
      ],
      "metadata": {
        "id": "OSfT6fhxFmOQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to return the SMAPE value\n",
        "def smape(A, F):\n",
        "    tmp = 2 * np.abs(F - A) / (np.abs(A) + np.abs(F))\n",
        "    len_ = np.count_nonzero(~np.isnan(tmp))\n",
        "    if len_ == 0 and np.nansum(tmp) == 0: # Deals with a special case\n",
        "        return 100\n",
        "    return round(100 / len_ * np.nansum(tmp), 3)"
      ],
      "metadata": {
        "id": "paOiln2fJPTx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import psutil\n",
        "\n",
        "# check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "print(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lQWv_I1qKQ09",
        "outputId": "b5934094-c370-476b-a65c-d9a96a5de5e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n",
            "CPU RAM Free: 10.94 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:440: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_hyperparameters(data, end_train, end_valid, exog_features,transformer_exog):\n",
        "\n",
        "  # instantiate a forcaster transformer with categorical features\n",
        "  forecaster = ForecasterAutoreg(\n",
        "  regressor = CatBoostRegressor(\n",
        "                  random_state=123,\n",
        "                  silent=True,\n",
        "                  allow_writing_files=False,\n",
        "                  boosting_type = 'Plain', # Faster training\n",
        "                  leaf_estimation_iterations = 3, # Faster training\n",
        "                  task_type = \"GPU\",\n",
        "                  devices = \"0\"\n",
        "              ),\n",
        "  lags = 24,\n",
        "  transformer_exog = transformer_exog\n",
        "  )\n",
        "\n",
        "\n",
        "  # search for best parameters\n",
        "  results_search, frozen_trial = bayesian_search_forecaster(\n",
        "  forecaster         = forecaster,\n",
        "  y                  = data.loc[:end_valid, 'gw-level'],\n",
        "  exog               = data.loc[:end_valid, exog_features],\n",
        "  search_space       = search_space,\n",
        "  steps              = 30,\n",
        "  refit              = False,\n",
        "  metric             = 'mean_absolute_percentage_error',\n",
        "  initial_train_size = len(data.loc[:end_train]),\n",
        "  fixed_train_size   = False,\n",
        "  n_trials           = 20,\n",
        "  random_state       = 123,\n",
        "  return_best        = True,\n",
        "  n_jobs             = 'auto',\n",
        "  verbose            = False,\n",
        "  show_progress      = True\n",
        "  )\n",
        "\n",
        "  best_params = results_search['params'].iat[0]\n",
        "\n",
        "  return best_params\n",
        "\n",
        "def train_and_predict(data, best_params, actual_data, end_valid, end_train, valid_num, train_num, df_idx, exog_features, transformer_exog):\n",
        "\n",
        "  # train for evaluation of the model\n",
        "  forecaster = ForecasterAutoreg(\n",
        "  regressor = CatBoostRegressor(**best_params,\n",
        "                  random_state=123,\n",
        "                  silent=True,\n",
        "                  allow_writing_files=False,\n",
        "                  boosting_type = 'Plain', # Faster training\n",
        "                  leaf_estimation_iterations = 3, # Faster training\n",
        "                  task_type = \"GPU\",\n",
        "                  devices = \"0\"\n",
        "              ),\n",
        "  lags = 24,\n",
        "  transformer_exog = transformer_exog\n",
        "  )\n",
        "\n",
        "  # train the model the time series train and validation dataset\n",
        "  forecaster.fit(\n",
        "    y    = data.loc[:end_train, 'gw-level'],\n",
        "    exog = data.loc[:end_train, exog_features]\n",
        "  )\n",
        "\n",
        "  # make predictions and evalute the model\n",
        "  predictions = forecaster.predict(\n",
        "      exog     = data.loc[df_idx[train_num+1]:, exog_features],\n",
        "      steps    = 26\n",
        "  )\n",
        "  df_preds = pd.DataFrame(predictions)\n",
        "  preds = df_preds[\"pred\"].values\n",
        "  # evaluate on symmetric mean absolute percentage error\n",
        "  smape_value = smape(actual_data, preds)\n",
        "\n",
        "  # train for future predictions\n",
        "  forecaster = ForecasterAutoreg(\n",
        "  regressor = CatBoostRegressor(**best_params,\n",
        "                  random_state=123,\n",
        "                  silent=True,\n",
        "                  allow_writing_files=False,\n",
        "                  boosting_type = 'Plain', # Faster training\n",
        "                  leaf_estimation_iterations = 3, # Faster training\n",
        "                  task_type = \"GPU\",\n",
        "                  devices = \"0\"\n",
        "              ),\n",
        "  lags = 24,\n",
        "  transformer_exog = transformer_exog\n",
        "  )\n",
        "\n",
        "  # train the model the time series train and validation dataset\n",
        "  forecaster.fit(\n",
        "    y    = data.loc[:end_valid, 'gw-level'],\n",
        "    exog = data.loc[:end_valid, exog_features]\n",
        "  )\n",
        "\n",
        "  # make predictions into the future\n",
        "  predictions = forecaster.predict(\n",
        "    exog     = data.loc[df_idx[valid_num+1]:, exog_features],\n",
        "    steps    = 26\n",
        "  )\n",
        "  df_preds = pd.DataFrame(predictions)\n",
        "\n",
        "  # free resources since it's going to run on iterations\n",
        "  del forecaster\n",
        "\n",
        "  return df_preds, smape_value"
      ],
      "metadata": {
        "id": "qJFpJzgfFmR5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def populate_test_data(data_dir):\n",
        "  preds_dict = {}\n",
        "  smape_dict = {}\n",
        "\n",
        "\n",
        "  # collect all files in the directory\n",
        "  filenames = os.listdir(data_dir)\n",
        "\n",
        "  try:\n",
        "\n",
        "    for filename in tqdm(filenames):\n",
        "\n",
        "        hrbnz01 = filename.split(\".\")[0].split(\"-\")[-1]\n",
        "        filepath = path.join(data_dir, filename)\n",
        "        df_exog = pd.read_csv(filepath)\n",
        "        df_exog[\"season\"] = df_exog[\"season\"].astype(\"category\")\n",
        "        df_exog[\"weather\"] = df_exog[\"weather\"].astype(\"category\")\n",
        "        df_exog[\"date\"] = pd.to_datetime(df_exog[\"date\"])\n",
        "        df_exog.set_index(\"date\", inplace=True)\n",
        "        df_exog.index = pd.date_range(start=df_exog.index.min(), end=df_exog.index.max(), freq='MS')\n",
        "\n",
        "        # get the estimate end train and end validation dates\n",
        "        data = df_exog.copy()\n",
        "        exog_data = data.drop(\"gw-level\", axis=1)\n",
        "        exog_features = exog_data.columns\n",
        "        df_idx = data.index\n",
        "        train_num = int(len(data) * 0.8)\n",
        "        valid_num = len(data.loc[:\"2021-11-01\"])\n",
        "        end_train = df_idx[train_num]\n",
        "        end_valid = df_idx[valid_num]\n",
        "        end_evaluation = df_idx[train_num+26]\n",
        "        evaluate_data = data.loc[df_idx[train_num+1]: end_evaluation, \"gw-level\"].values\n",
        "\n",
        "\n",
        "        # tune for best hyperparamters and evaluate on MAPE metric\n",
        "        best_params = search_hyperparameters(data, end_train, end_valid, exog_features, transformer_exog)\n",
        "\n",
        "        # train and make predict into 26 months in the future of the test template\n",
        "        df_predictions, smape = train_and_predict(data,best_params, evaluate_data, end_valid, end_train, valid_num, train_num, df_idx, exog_features, transformer_exog)\n",
        "        df_predictions[\"pred\"] =  df_predictions[\"pred\"].round(2)\n",
        "        preds_dict[hrbnz01] = df_predictions['pred'].values\n",
        "        smape_dict[hrbnz01] = smape\n",
        "\n",
        "\n",
        "  except Exception as ex:\n",
        "    print(\"[Error]\")\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "  df_smape = pd.DataFrame(smape_dict, index=[0])\n",
        "  df_final_preds = pd.DataFrame(preds_dict, index=range(26))\n",
        "  df_final_preds.index = pd.date_range(start=pd.Timestamp(\"2021-12-01\") + pd.DateOffset(months=1), periods=26, freq='MS')\n",
        "  print(\"> Done\")\n",
        "\n",
        "  return df_final_preds, df_smape"
      ],
      "metadata": {
        "id": "GBQNJyBuFmY7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data_dir = \"/content/drive/MyDrive/clean_processed_data_part1\"\n",
        "\n",
        "df_submission, df_smape = populate_test_data(processed_data_dir)\n",
        "\n",
        "df_submission.to_csv(\"df_submission.csv\")\n",
        "df_smape.to_csv(\"smape_score.csv\")"
      ],
      "metadata": {
        "id": "Pdu2ZdfIFmsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}